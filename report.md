# Article Retrieval System: Report

This report discusses the design, technologies used, challenges encountered, and potential areas for future development of an Article Retrieval System.

## Introduction

#### This system focuses on two primary objectives:
   - Constructing a system designed to retrieve relevant article fragments according to a given query.
   - Utilizing the retrieved information to enhance an input to a generative model to develop a RAG Q&A system.

The articles are from the [1300+ Towards DataScience Medium Articles Dataset](https://www.kaggle.com/datasets/meruvulikith/1300-towards-datascience-medium-articles-dataset) 

## System design

#### The system consists of several key components:
**Preprocessing and Embedding:**
   - Articles are split into chunks using the Recursive Text Splitter.
   - These chunks are embedded and normalized using the `gte-large-en-v1.5`embedding model, then stored into a tensor.

**Retrieval**
   - The query is embedded using the same embedding model
   - The relevant arictle fragments are retrieved based on a query using the vector search for embeddings tensors
   - These fragments are then reranked using the `mxbai-rerank-large-v1` reranking model
   
**RAG Q&A system**
   - The `gemma-1.1-7b-it` LLM model is utilized for text generation. 
   - The context items are retrieved as in the above Retrieval step
   - The input prompt is formatted for generating answers based on a given query and context items.

## Technologies Used

The development of the Article Retrieval System relied on several key technologies and libraries:

- Recursive Text Splitter: Used for splitting articles into smaller chunks.
- Embedding Model (gte-large-en-v1.5): Utilized for embedding and normalizing the article chunks.
- Vector Search for Embeddings Tensors: Employed for retrieving relevant article fragments based on a query.
- Reranking Model (mxbai-rerank-large-v1): Used for reranking the retrieved article fragments.
- Generative Language Model (gemma-1.1-7b-it): Employed for text generation in the RAG Q&A system.
- Frameworks: LangChain, PyTorch, Transformers, Sentence-transformers


## Challenges Encountered

Throughout the development process of the Article Retrieval System, several challenges were encountered, each requiring careful consideration and resolution:

1. **Choosing the LLM Model**: Initially, the challenge revolved around selecting a language model suitable for the task while not having access to models requiring API keys. This limitation led to the utilization of Kaggle notebooks, providing an environment conducive to running the chosen LLM.

2. **Choosing a Chunk Size**: Finding the optimal balance between relevance and granularity posed another challenge. After experimenting with various chunk sizes, I concluded that a chunk size of 1024 works best. This size ensures that separate contexts are stored in individual chunks.

3. **Choosing the Embedding Model**: Selecting an appropriate embedding model was crucial for effective representation of article chunks. I've chosen the `gte-large-en-v1.5` model for its lower memory usage compared to other models, while also demonstrating strong performance. Currently, it holds the 9th position on the leaderboard, which can be viewed [here](https://huggingface.co/spaces/mteb/leaderboard).

4. **Choosing the Prompt for LLM Augmentation**: Experimentation with different prompts was necessary to enhance the quality of answers generated by the LLM. The following prompt emerged as the most effective (you can see the full prompt at the Jupyter notebook):

   "Based on the following context items, please provide a comprehensive answer to the query.
Give yourself room to think by extracting relevant passages from the context before answering the query.
Don't return the thinking, only return the answer.
Make sure your answers are as explanatory as possible.
Use the following examples as reference for the ideal answer style.

   Example 1:

   Query: ...
   
   Answer: ...
   
   ...
   
   Example 3:
   
   ...
   
   Now use the following context items to answer the user query:
   
   {context}
   
   User query: {query}
   
   Answer:"
   

## Future Development

- **Performance Optimization**: Investigate techniques to enhance the performance and efficiency of chunking, embedding, and reranking processes, especially for larger setups involving over 100,000 chunks. Consider utilizing the Vector database/index for improved scalability and speed.

- **OOP implementation**: Develop a class structure to organize and manage the various components and processes involved in the system's tasks. This will improve code modularity, reusability, and maintainability.

- **Expansion of Data Sources**: Integrate additional datasets to enrich the content available for retrieval. By expanding the variety and coverage of articles, the system can provide more comprehensive and diverse responses to user queries.

- **Fine-Tuning the LLM**: Explore opportunities for fine-tuning the LLM to better suit the specific requirements of the system, potentially improving its accuracy and relevance in generating responses.

- **Evaluation of Answers**: Employ another LLM to assess and rate the responses, providing valuable feedback for further refinement.

- **Enhancement of LLM Memory**: Investigate methods to enable the LLM to remember the history of queries and interactions. This would facilitate a more context-aware and personalized response generation process.

- **Handling Irrelevant Queries**: Develop mechanisms to identify and appropriately handle queries that are irrelevant to the articles dataset, ensuring the system provides accurate and helpful responses even in such cases.

- **User Interface Development**: Create a user-friendly interface to streamline interaction with the system. This should enhance the overall usability and accessibility of the system.
