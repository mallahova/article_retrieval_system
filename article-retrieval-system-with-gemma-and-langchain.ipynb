{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8311145,"sourceType":"datasetVersion","datasetId":4937245},{"sourceId":8320808,"sourceType":"datasetVersion","datasetId":4942573},{"sourceId":26154,"sourceType":"modelInstanceVersion","modelInstanceId":22015}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Article Retrieval System with Gemma and LangChain\n\n## Introduction\n\n#### This notebook focuses on two primary objectives:\n   - Constructing a system designed to retrieve relevant article fragments according to a given query.\n   - Utilizing the retrieved information to enhance an input to a generative model to develop a RAG Q&A system.\n\nI'm going to use the articles from the [1300+ Towards DataScience Medium Articles Dataset](https://www.kaggle.com/datasets/meruvulikith/1300-towards-datascience-medium-articles-dataset) ","metadata":{}},{"cell_type":"markdown","source":"## Installation and imports\n","metadata":{}},{"cell_type":"code","source":"!pip install -q -U transformers accelerate bitsandbytes langchain sentence-transformers","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-06T01:16:55.945159Z","iopub.execute_input":"2024-05-06T01:16:55.945556Z","iopub.status.idle":"2024-05-06T01:17:10.890733Z","shell.execute_reply.started":"2024-05-06T01:16:55.945526Z","shell.execute_reply":"2024-05-06T01:17:10.889320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import DataFrameLoader\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig\nfrom sentence_transformers import SentenceTransformer, util,CrossEncoder\nfrom IPython.display import display, Markdown\nimport logging\nimport warnings","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:17:17.495359Z","iopub.execute_input":"2024-05-06T01:17:17.496329Z","iopub.status.idle":"2024-05-06T01:17:17.503229Z","shell.execute_reply.started":"2024-05-06T01:17:17.496284Z","shell.execute_reply":"2024-05-06T01:17:17.502019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hide warnings\nwarnings.filterwarnings('ignore')\n# Disable logging for transformers library\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\n# Disable logging for XLA (Accelerated Linear Algebra) library\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:13:49.467330Z","iopub.execute_input":"2024-05-06T01:13:49.468048Z","iopub.status.idle":"2024-05-06T01:13:49.473834Z","shell.execute_reply.started":"2024-05-06T01:13:49.468017Z","shell.execute_reply":"2024-05-06T01:13:49.472911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data cleaning and exploration","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/medium/medium.csv')\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:13:50.744679Z","iopub.execute_input":"2024-05-06T01:13:50.745413Z","iopub.status.idle":"2024-05-06T01:13:50.998397Z","shell.execute_reply.started":"2024-05-06T01:13:50.745379Z","shell.execute_reply":"2024-05-06T01:13:50.997424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling duplicate Article Titles","metadata":{"execution":{"iopub.status.busy":"2024-05-06T00:23:57.091071Z","iopub.execute_input":"2024-05-06T00:23:57.091827Z","iopub.status.idle":"2024-05-06T00:23:57.097887Z","shell.execute_reply.started":"2024-05-06T00:23:57.091788Z","shell.execute_reply":"2024-05-06T00:23:57.096498Z"}}},{"cell_type":"code","source":"mask = df.groupby('Title')['Title'].transform('size') > 1\nnon_unique_rows = df[mask]\nnon_unique_rows","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:13:51.415554Z","iopub.execute_input":"2024-05-06T01:13:51.416437Z","iopub.status.idle":"2024-05-06T01:13:51.437634Z","shell.execute_reply.started":"2024-05-06T01:13:51.416404Z","shell.execute_reply":"2024-05-06T01:13:51.436555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,row in enumerate(non_unique_rows['Text']):\n    print(f'Text {i}')\n    print(row)\n    print('--------------------------------------------------------------------------')","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-05-06T01:13:51.760025Z","iopub.execute_input":"2024-05-06T01:13:51.760669Z","iopub.status.idle":"2024-05-06T01:13:51.766676Z","shell.execute_reply.started":"2024-05-06T01:13:51.760637Z","shell.execute_reply":"2024-05-06T01:13:51.765564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove the duplicate row - the texts with the same article title are identical.","metadata":{}},{"cell_type":"code","source":"df = df.drop(616)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:13:52.330645Z","iopub.execute_input":"2024-05-06T01:13:52.331504Z","iopub.status.idle":"2024-05-06T01:13:52.339381Z","shell.execute_reply.started":"2024-05-06T01:13:52.331469Z","shell.execute_reply":"2024-05-06T01:13:52.338203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove emojis","metadata":{}},{"cell_type":"code","source":"def remove_emojis(text):\n        emoji_pattern = re.compile(\"[\"\n                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                                   \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', text) # no emoji\n# df.apply(remove_emojis)\ndf['Title'] = df['Title'].apply(remove_emojis)\ndf['Text'] = df['Text'].apply(remove_emojis)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:13:52.875782Z","iopub.execute_input":"2024-05-06T01:13:52.876472Z","iopub.status.idle":"2024-05-06T01:13:53.363620Z","shell.execute_reply.started":"2024-05-06T01:13:52.876439Z","shell.execute_reply":"2024-05-06T01:13:53.362780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n### Explore the data","metadata":{}},{"cell_type":"code","source":"df_stats=df.copy()\ndf_stats['Text_length'] = df['Text'].apply(len)\ndf_stats['Words_count'] = df['Text'].apply(lambda x: len(x.split(\" \")))\ndf_stats['Sentence_count'] = df['Text'].apply(lambda x: len(x.split(\". \")))\ndf_stats['Token_count']=df['Text'].apply(len)/4\ndf_stats.plot(y='Text_length')\ndf_stats.describe()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:13:53.463120Z","iopub.execute_input":"2024-05-06T01:13:53.463865Z","iopub.status.idle":"2024-05-06T01:13:54.306961Z","shell.execute_reply.started":"2024-05-06T01:13:53.463818Z","shell.execute_reply":"2024-05-06T01:13:54.305915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Chunking\n### I'll utilize a `Recursive Text Splitter`. Splitting text recursively serves the purpose of trying to keep related pieces of text next to each other. \n\n### After conducting experiments with different chunk sizes, I've determined that a chunk size of 1024 works best. This size effectively captures context and is likely to contain all necessary information within the top chunks.","metadata":{}},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 1024,\n    chunk_overlap  = 100\n)\n\narticles = DataFrameLoader(df, page_content_column = \"Text\")\ndocument = articles.load()\ntext_chunks= text_splitter.split_documents(document)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:13:54.309122Z","iopub.execute_input":"2024-05-06T01:13:54.309817Z","iopub.status.idle":"2024-05-06T01:13:54.989166Z","shell.execute_reply.started":"2024-05-06T01:13:54.309781Z","shell.execute_reply":"2024-05-06T01:13:54.988174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,chunk in enumerate(text_chunks[:3]):\n   print(f'Chunk {i}')\n   print(chunk.metadata)\n   print(chunk.page_content)\n   print('---------------------------------------------------------------------------------')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-06T01:13:54.990834Z","iopub.execute_input":"2024-05-06T01:13:54.991168Z","iopub.status.idle":"2024-05-06T01:13:54.997185Z","shell.execute_reply.started":"2024-05-06T01:13:54.991142Z","shell.execute_reply":"2024-05-06T01:13:54.996196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Convert chunk document to a DataFrame","metadata":{}},{"cell_type":"code","source":"chunks_df=pd.DataFrame()\nchunks_df['Chunk']=[chunk.page_content for chunk in text_chunks]\nchunks_df['Title']=[chunk.metadata['Title'] for chunk in text_chunks]\nchunks_list=list(chunks_df['Chunk'])\nchunks_df","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:13:55.382516Z","iopub.execute_input":"2024-05-06T01:13:55.383412Z","iopub.status.idle":"2024-05-06T01:13:55.410567Z","shell.execute_reply.started":"2024-05-06T01:13:55.383366Z","shell.execute_reply":"2024-05-06T01:13:55.409471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embedding\n### I'll be utilizing the `gte-large-en-v1.5` model for embedding purposes. This specific model has been chosen for its lower memory usage compared to other models, while also demonstrating strong performance. Currently, it holds the 9th position on the leaderboard, which can be viewed [here](https://huggingface.co/spaces/mteb/leaderboard).\n","metadata":{}},{"cell_type":"code","source":"embedding_model = SentenceTransformer('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True )","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:13:56.758206Z","iopub.execute_input":"2024-05-06T01:13:56.758575Z","iopub.status.idle":"2024-05-06T01:14:11.278046Z","shell.execute_reply.started":"2024-05-06T01:13:56.758548Z","shell.execute_reply":"2024-05-06T01:14:11.276606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: Creating the embeddings using this model takes some time, therefore I've saved them to a csv file. You can uncomment this cell to get the embeddings without a csv.","metadata":{}},{"cell_type":"code","source":"embeddings = embedding_model.encode(chunks_df['Chunk'],convert_to_tensor=True,normalize_embeddings=True)\n# embeddings_df = pd.DataFrame(embeddings.tolist())\n# embeddings_df_save_path = \"embeddings_df.csv\"\n# embeddings_df.to_csv(embeddings_df_save_path, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:14:11.280717Z","iopub.execute_input":"2024-05-06T01:14:11.281272Z","iopub.status.idle":"2024-05-06T01:14:11.291372Z","shell.execute_reply.started":"2024-05-06T01:14:11.281221Z","shell.execute_reply":"2024-05-06T01:14:11.290157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embeddings_df=pd.read_csv('/kaggle/input/embeddings/embeddings_df.csv')\n# embeddings = torch.tensor(embeddings_df.values, dtype=torch.float32, device='cuda')\n# embeddings","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:14:11.294682Z","iopub.execute_input":"2024-05-06T01:14:11.297266Z","iopub.status.idle":"2024-05-06T01:14:18.594591Z","shell.execute_reply.started":"2024-05-06T01:14:11.297206Z","shell.execute_reply":"2024-05-06T01:14:18.593575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:14:18.603626Z","iopub.execute_input":"2024-05-06T01:14:18.603914Z","iopub.status.idle":"2024-05-06T01:14:18.609667Z","shell.execute_reply.started":"2024-05-06T01:14:18.603889Z","shell.execute_reply":"2024-05-06T01:14:18.608853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Retrieval Without Reranking\n","metadata":{}},{"cell_type":"code","source":"def top_k_results_with_scores(query, embeddings,chunks_df, k_=5, embedding_model=embedding_model):\n    \"\"\"\n    Calculates the dot product similarity scores between the query embedding and the embeddings of chunks in the corpus.\n    Returns the top k_ chunks along with their corresponding similarity scores.\n\n    Parameters:\n    - query: The query string.\n    - embeddings: Embeddings of the chunks.\n    - chunks_df: DataFrame containing the chunks.\n    - k_: Number of top chunks to retrieve (default is 5).\n    - embedding_model: The model used for embedding (default is embedding_model).\n\n    Returns:\n    - A generator of tuples containing the top k_ chunks from chunks_df along with their dot product similarity scores.\n      Each tuple consists of a chunk (DataFrame row) and its corresponding score.\n    \"\"\"\n    query_embedding=embedding_model.encode(query,convert_to_tensor=True,normalize_embeddings=True)\n    dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n    top_results_dot_product = torch.topk(dot_scores, k=k_)\n    return zip([chunks_df.loc[i] for i in top_results_dot_product[1].tolist()], top_results_dot_product[0].tolist())","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:14:18.610605Z","iopub.execute_input":"2024-05-06T01:14:18.610868Z","iopub.status.idle":"2024-05-06T01:14:21.146900Z","shell.execute_reply.started":"2024-05-06T01:14:18.610817Z","shell.execute_reply":"2024-05-06T01:14:21.145908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def top_k_results(query, embeddings,chunks_df, k_=5,embedding_model=embedding_model):\n    \"\"\"\n    Calculates the dot product similarity scores between the query embedding and the embeddings of chunks in the corpus.\n    Returns the top k_ chunks.\n\n    Parameters:\n    - query: The query string.\n    - embeddings: Embeddings of the chunks.\n    - chunks_df: DataFrame containing the chunks.\n    - k_: Number of top chunks to retrieve (default is 5).\n    - embedding_model: The model used for embedding (default is embedding_model).\n\n    Returns:\n    - DataFrame containing the top k_ chunks from chunks_df.\n    \"\"\"\n    query_embedding=embedding_model.encode(query,convert_to_tensor=True,normalize_embeddings=True)\n    dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n    top_results_dot_product = torch.topk(dot_scores, k=k_)\n    top_k=chunks_df.loc[top_results_dot_product[1].tolist()]\n    top_k.reset_index(drop=True, inplace=True)\n    return top_k","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:14:21.148490Z","iopub.execute_input":"2024-05-06T01:14:21.148827Z","iopub.status.idle":"2024-05-06T01:14:21.157592Z","shell.execute_reply.started":"2024-05-06T01:14:21.148797Z","shell.execute_reply":"2024-05-06T01:14:21.156731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Example query","metadata":{}},{"cell_type":"code","source":"query='What is the curse of dimensionality?'\nfor chunk, score in top_k_results_with_scores(query, embeddings,chunks_df):\n    print(f'Score: {score}')\n    print(\"Title: \"+ chunk['Title']+'\\n')\n    print(\"Text:\")\n    print(chunk['Chunk']+'\\n')\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-06T01:14:21.158657Z","iopub.execute_input":"2024-05-06T01:14:21.160420Z","iopub.status.idle":"2024-05-06T01:14:22.197992Z","shell.execute_reply.started":"2024-05-06T01:14:21.160383Z","shell.execute_reply":"2024-05-06T01:14:22.196893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reranking\n### Reranking serves the purpose of reordering and refining a set of retrieved article fragments based on their relevance to a given query.\n","metadata":{}},{"cell_type":"code","source":"reranking_model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-large-v1\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:14:22.200466Z","iopub.execute_input":"2024-05-06T01:14:22.200771Z","iopub.status.idle":"2024-05-06T01:14:31.384758Z","shell.execute_reply.started":"2024-05-06T01:14:22.200744Z","shell.execute_reply":"2024-05-06T01:14:31.383604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Searching for relevant article fragments according to a given query","metadata":{}},{"cell_type":"code","source":"def top_k_results_reranked(query,embeddings, chunks_df, embedding_model=embedding_model, reranking_model=reranking_model, k1=15, k2=5 ):\n    \"\"\"\n    Embeds a query with the specified model and retrieves the top k1 relevant article fragments (chunks) using semantic search.\n    Then, it reranks these chunks using the reranking model and returns the top k2 article fragments along with their scores.\n\n    Parameters:\n    - query: The query string.\n    - embeddings: Embeddings of the chunks.\n    - chunks_df: DataFrame containing the chunks.\n    - embedding_model: The model used for embedding (default is embedding_model).\n    - reranking_model: The model used for reranking (default is reranking_model).\n    - k1: Number of top relevant chunks to retrieve initially (default is 10).\n    - k2: Number of top reranked chunks to return (default is 5).\n\n    Returns:\n    - top_k_reranked: DataFrame containing the top k2 reranked chunks.\n    - results['score']: Series containing the scores of the top k2 reranked chunks.\n    \"\"\"\n    top_k=top_k_results(query, embeddings,chunks_df, embedding_model=embedding_model, k_=k1)\n    results = reranking_model.rank(query, list(top_k['Chunk']), return_documents=False, top_k=k2)\n    results=pd.DataFrame(results)\n    index=(results['corpus_id'])\n    top_k_reranked=top_k.loc[index]\n    top_k_reranked.reset_index(drop=True, inplace=True)\n    return top_k_reranked, results['score']","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:14:31.386713Z","iopub.execute_input":"2024-05-06T01:14:31.387340Z","iopub.status.idle":"2024-05-06T01:14:31.397410Z","shell.execute_reply.started":"2024-05-06T01:14:31.387311Z","shell.execute_reply":"2024-05-06T01:14:31.395729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_top_k_results_reranked(query,embeddings, chunks_df, embedding_model=embedding_model, reranking_model=reranking_model, k1=10, k2=5 ):\n    \"\"\"\n    Prints out the results from the function top_k_results_reranked\n    \"\"\"\n    top_k_reranked,scores=top_k_results_reranked(query, embeddings, chunks_df, k1=10, k2=5 )\n    for i, chunk in top_k_reranked.iterrows():\n        print(f'Score: {scores[i]}')\n        print(\"Title: \"+ chunk['Title'])\n        print(\"Text:\")\n        print(chunk['Chunk']+'\\n')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:14:31.398595Z","iopub.execute_input":"2024-05-06T01:14:31.398997Z","iopub.status.idle":"2024-05-06T01:14:31.630776Z","shell.execute_reply.started":"2024-05-06T01:14:31.398960Z","shell.execute_reply":"2024-05-06T01:14:31.629488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Get results for the same example query with reranking","metadata":{}},{"cell_type":"code","source":"query='What is the curse of dimensionality?'\nprint_top_k_results_reranked(query, embeddings, chunks_df, k1=10, k2=5 )","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-06T01:14:31.633094Z","iopub.execute_input":"2024-05-06T01:14:31.636579Z","iopub.status.idle":"2024-05-06T01:14:32.797115Z","shell.execute_reply.started":"2024-05-06T01:14:31.636544Z","shell.execute_reply":"2024-05-06T01:14:32.796052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comparing the results, it's evident that reranking significantly improves the relevance of retrieved passages, with higher-scoring passages aligning more closely with the query's intent. This highlights the effectiveness of rerankers in enhancing the quality of retrieved content for RAG models, ensuring more accurate and contextually appropriate responses.","metadata":{}},{"cell_type":"markdown","source":"#### More examples","metadata":{}},{"cell_type":"code","source":"query='How to handle outliers in a dataset'\nprint_top_k_results_reranked(query, embeddings, chunks_df, k1=15, k2=5 )","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-06T01:14:32.810813Z","iopub.execute_input":"2024-05-06T01:14:32.811208Z","iopub.status.idle":"2024-05-06T01:14:33.578831Z","shell.execute_reply.started":"2024-05-06T01:14:32.811180Z","shell.execute_reply":"2024-05-06T01:14:33.577783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query='How does gradient descent work?'\nprint_top_k_results_reranked(query, embeddings, chunks_df, k1=15, k2=5 )","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-06T01:14:33.580066Z","iopub.execute_input":"2024-05-06T01:14:33.580393Z","iopub.status.idle":"2024-05-06T01:14:34.539195Z","shell.execute_reply.started":"2024-05-06T01:14:33.580365Z","shell.execute_reply":"2024-05-06T01:14:34.538211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  LLM Model\n#### I'll utilize a `gemma-1.1-7b-it` model for text generation.","metadata":{}},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/gemma/transformers/1.1-7b-it/1/\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"/kaggle/input/gemma/transformers/1.1-7b-it/1/\",\n    quantization_config=quantization_config,\n    low_cpu_mem_usage=False,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:17:22.808151Z","iopub.execute_input":"2024-05-06T01:17:22.808536Z","iopub.status.idle":"2024-05-06T01:17:24.119353Z","shell.execute_reply.started":"2024-05-06T01:17:22.808506Z","shell.execute_reply":"2024-05-06T01:17:24.117884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing the model without RAG","metadata":{}},{"cell_type":"code","source":"query=\"What are some common cases for using on-device deep learning with TensorFlow Mobile?\"\nchat = [\n    { \"role\": \"user\", \"content\": query },\n]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids,max_new_tokens=256)\nanswer = (tokenizer.decode(outputs[0]))\ndisplay(Markdown(answer.replace(prompt, '')))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-06T00:13:58.686394Z","iopub.execute_input":"2024-05-06T00:13:58.686740Z","iopub.status.idle":"2024-05-06T00:14:32.923532Z","shell.execute_reply.started":"2024-05-06T00:13:58.686709Z","shell.execute_reply":"2024-05-06T00:14:32.922423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Q&A system","metadata":{"execution":{"iopub.status.busy":"2024-05-06T09:17:17.405003Z","iopub.execute_input":"2024-05-06T09:17:17.405363Z","iopub.status.idle":"2024-05-06T09:17:17.410077Z","shell.execute_reply.started":"2024-05-06T09:17:17.405341Z","shell.execute_reply":"2024-05-06T09:17:17.409008Z"}}},{"cell_type":"code","source":"def format_prompt(query, context_items) -> str:\n    \"\"\"\n    Formats a prompt for generating answers based on a given query and context items.\n    \"\"\"\n    context= \"- \"+ \"\\n- \".join(context_items)\n    base_prompt = \"\"\"Based on the following context items, please provide a comprehensive answer to the query.\nGive yourself room to think by extracting relevant passages from the context before answering the query.\nDon't return the thinking, only return the answer.\nMake sure your answers are as explanatory as possible.\nUse the following examples as reference for the ideal answer style.\n\\nExample 1:\nQuery: What are some commonly used evaluation metrics for classification tasks?\nAnswer: Some commonly used evaluation metrics for classification tasks include:\n1. Accuracy: It measures the proportion of correct predictions among the total number of cases examined. It's suitable for well-balanced classification problems without class imbalance.\n2. Precision: It measures the proportion of true positive predictions among all positive predictions made by the model. It's useful when the cost of false positives is high.\n3. Recall (Sensitivity): It measures the proportion of true positive predictions among all actual positive cases in the dataset. It's important when the cost of false negatives is high.\n4. F1-score: It is the harmonic mean of precision and recall, providing a balance between the two metrics. It's helpful when there's an uneven class distribution or when both false positives and false negatives are important.\n5. ROC (Receiver Operating Characteristic) curve: It plots the true positive rate (sensitivity) against the false positive rate (1-specificity) at various threshold settings. It's useful for assessing the trade-off between true positive rate and false positive rate.\n6. AUC (Area Under the ROC Curve): It quantifies the overall performance of a binary classification model by calculating the area under the ROC curve. A higher AUC indicates better model performance.\nThese metrics provide insights into different aspects of a classification model's performance and are commonly used to evaluate and compare models in various applications.\n\\nExample 2:\nQuery: What is the role of cross-validation in model evaluation?\nAnswer: Cross-validation is a technique used to assess how well a predictive model will generalize to an independent dataset. It involves partitioning the dataset into multiple subsets, called folds, training the model on several combinations of these folds, and evaluating its performance on the remaining fold(s). This process helps to detect overfitting by providing a more accurate estimate of the model's performance on unseen data compared to a single train-test split. Common cross-validation methods include k-fold cross-validation and stratified k-fold cross-validation, which ensure that each fold preserves the class distribution of the original dataset. Cross-validation is essential for robust model evaluation and hyperparameter tuning in machine learning projects.\n\\nExample 3:\nQuery: How does regularization help in preventing overfitting in machine learning models?\nAnswer: Regularization is a technique used to prevent overfitting in machine learning models by adding a penalty term to the loss function. This penalty term discourages complex models that fit the training data too closely, thus reducing the likelihood of overfitting. Common regularization techniques include L1 regularization (Lasso), L2 regularization (Ridge), and elastic net regularization, each of which adds a different type of penalty to the model's weights. Regularization helps to improve the model's generalization performance by balancing between fitting the training data well and avoiding excessive complexity.\n\\nNow use the following context items to answer the user query:\n{context}\n\\nRelevant passages: <extract relevant passages from the context here>\nUser query: {query}\nAnswer:\"\"\"\n    prompt=base_prompt.format(context=context, query=query)\n    return prompt","metadata":{"execution":{"iopub.status.busy":"2024-05-06T00:14:32.927845Z","iopub.execute_input":"2024-05-06T00:14:32.928385Z","iopub.status.idle":"2024-05-06T00:14:32.938370Z","shell.execute_reply.started":"2024-05-06T00:14:32.928357Z","shell.execute_reply":"2024-05-06T00:14:32.937492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ask(query, temperature=0.7,max_new_tokens=512, embeddings=embeddings, chunks_df=chunks_df):\n    \"\"\"\n    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n    \"\"\"\n    context_items,_= top_k_results_reranked(query,embeddings, chunks_df)\n    input_text=format_prompt(query,list(context_items[\"Chunk\"]))\n    chat = [\n    { \"role\": \"user\", \"content\": input_text },\n    ]\n    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    outputs = model.generate(**input_ids,max_new_tokens=max_new_tokens, temperature=temperature)\n    answer = (tokenizer.decode(outputs[0]))\n    return answer.replace(prompt, '').replace('<bos>', '').replace('<eos>', ''), context_items\n\ndef print_context_items(context_items):\n    print(\"Retrieved article fragments:\")\n    for _, chunk in context_items.iterrows():\n        print(\"---------------------------------------------------------------------------------------\")\n        print(\"Title: \"+ chunk['Title'])\n        print(\"Text:\")\n        print(chunk['Chunk']+'\\n')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T09:05:22.513273Z","iopub.execute_input":"2024-05-06T09:05:22.513597Z","iopub.status.idle":"2024-05-06T09:05:22.767502Z","shell.execute_reply.started":"2024-05-06T09:05:22.513574Z","shell.execute_reply":"2024-05-06T09:05:22.766199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get results for the same query with RAG","metadata":{}},{"cell_type":"code","source":"query=\"What are some common cases for using on-device deep learning with TensorFlow Mobile?\"\nanswer, context_items=ask(query)\nprint(f'Query: {query}')\ndisplay(Markdown(answer))\nprint_context_items(context_items)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-06T00:14:33.019497Z","iopub.execute_input":"2024-05-06T00:14:33.020383Z","iopub.status.idle":"2024-05-06T00:14:58.369511Z","shell.execute_reply.started":"2024-05-06T00:14:33.020348Z","shell.execute_reply":"2024-05-06T00:14:58.368616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The variation in output between the two responses highlights the impact of using RAG. The second response focused specifically on common cases for on-device deep learning with TensorFlow Mobile, emphasizing applications like image recognition and text classification. In contrast, the first response, without RAG, covered a broader range of use cases. This demonstrates how RAG can yield more targeted and contextually relevant responses from a specific domain of knowledge.","metadata":{}},{"cell_type":"markdown","source":"### More query examples","metadata":{}},{"cell_type":"code","source":"query = \"Can you explain PCA?\"\nanswer, context_items=ask(query)\nprint(f'Query: {query}')\ndisplay(Markdown(answer))\nprint_context_items(context_items)","metadata":{"_kg_hide-input":false,"scrolled":true,"execution":{"iopub.status.busy":"2024-05-06T00:14:58.370891Z","iopub.execute_input":"2024-05-06T00:14:58.371539Z","iopub.status.idle":"2024-05-06T00:15:43.897369Z","shell.execute_reply.started":"2024-05-06T00:14:58.371486Z","shell.execute_reply":"2024-05-06T00:15:43.896470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"What is the purpose of feature scaling in machine learning?\"\nanswer, context_items=ask(query)\nprint(f'Query: {query}')\ndisplay(Markdown(answer))\nprint_context_items(context_items)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-06T00:15:43.898817Z","iopub.execute_input":"2024-05-06T00:15:43.899175Z","iopub.status.idle":"2024-05-06T00:16:24.328527Z","shell.execute_reply.started":"2024-05-06T00:15:43.899140Z","shell.execute_reply":"2024-05-06T00:16:24.327493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query=\"What are some ethical considerations in data science, particularly regarding privacy and bias?\"\nanswer, context_items=ask(query)\nprint(f'Query: {query}')\ndisplay(Markdown(answer))\nprint_context_items(context_items)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-06T00:16:24.330046Z","iopub.execute_input":"2024-05-06T00:16:24.330733Z","iopub.status.idle":"2024-05-06T00:17:15.832588Z","shell.execute_reply.started":"2024-05-06T00:16:24.330696Z","shell.execute_reply":"2024-05-06T00:17:15.831709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query=\"Explain how k-nearest neighbors (k-NN) algorithm works and discuss its limitations.\"\nanswer, context_items=ask(query)\nprint(f'Query: {query}')\ndisplay(Markdown(answer))\nprint_context_items(context_items)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-06T00:17:15.835105Z","iopub.execute_input":"2024-05-06T00:17:15.835403Z","iopub.status.idle":"2024-05-06T00:18:12.009503Z","shell.execute_reply.started":"2024-05-06T00:17:15.835378Z","shell.execute_reply":"2024-05-06T00:18:12.008574Z"},"trusted":true},"execution_count":null,"outputs":[]}]}